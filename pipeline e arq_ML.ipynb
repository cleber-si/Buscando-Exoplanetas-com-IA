{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Para Testes\n",
    "Faça seus testes aqui, se preferir. Procure por \"insira/o/caminho\" para selecionar e definir os locais das pastas e arquivos. Uma vez definidos esses caminhos corretamente, todo o código será executado automaticamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tratamento import kepler_io\n",
    "from tratamento import processos as pc\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import itertools  # Para transformar arrays em arrays unidimensionais\n",
    "import pandas as pd\n",
    "\n",
    "import _pickle as cPickle  # Para salvar os arquivos\n",
    "import traceback  # Para armazenar logs de erro\n",
    "\n",
    "# Para plotar figuras mais bonitas\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "plt.rcParams[\"figure.figsize\"] = (16,8)\n",
    "plt.rcParams.update({'font.size': 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1 # Número da pasta e do processo para ser executado\n",
    "pasta = str(n)\n",
    "salva_arq = \"insira/o/caminho\" + pasta\n",
    "\"\"\" \n",
    "Exemplo:\n",
    "(Linux)   raiz = \"/home/você/Documentos/kepler_separados/kepler_\" + pasta\n",
    "(Windows) raiz = \"C:\\\\Users\\\\você\\\\Documentos\\\\kepler_separados\\\\kepler_\" + pasta\n",
    "\"\"\"\n",
    "\n",
    "PASTA_DADOS_KEPLER = \"insira/o/caminho\" + pasta\n",
    "PASTA_ARQ_CSV = \"insira/o/caminho\"\n",
    "\n",
    "# Cria a pasta para armazenar os arquivos salvos caso não exista.\n",
    "if not os.path.exists(salva_arq):\n",
    "    os.makedirs(salva_arq)\n",
    "\n",
    "# Lê todas as pastas da pasta principal dos dados baixados do kepler e armazena em sub_ids\n",
    "# Seus nomes correspondem aos primeiros quatro dígitos do Kepler_ID\n",
    "sub_ids = [item for item in os.listdir(PASTA_DADOS_KEPLER) if os.path.isdir(\n",
    "    os.path.join(PASTA_DADOS_KEPLER, item))]\n",
    "\n",
    "# Lista com todas as subpastas da pasta raiz dos dados\n",
    "sub_pastas = [os.path.join(PASTA_DADOS_KEPLER, item) for item in sub_ids]\n",
    "\n",
    "IDs = []                  # Lista com todos o Kepler_IDs\n",
    "pastas_TCEs = []          # Lista com todos as pastas dos TCEs\n",
    "for i in sub_pastas:\n",
    "    pastas_TCEs.append(\n",
    "        [os.path.join(i, item) for item in os.listdir(i) if os.path.isdir(os.path.join(i, item))])\n",
    "    IDs.append([item for item in os.listdir(i) if os.path.isdir(os.path.join(i, item))])\n",
    "\n",
    "pastas_TCEs = list(itertools.chain(*pastas_TCEs))\n",
    "IDs = list(itertools.chain(*IDs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cria um dataframe do Pandas com base no arquivo \"dr24_tce.csv\"\n",
    "tabela_tce = pd.read_csv(PASTA_ARQ_CSV, index_col=\"rowid\", comment=\"#\")\n",
    "\n",
    "'''\n",
    "# Remove colunas obsoletas (remova qualquer coluna que não for usar)\n",
    "tabela_tce = tabela_tce.drop(columns = ['Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12'])\n",
    "'''\n",
    "\n",
    "# Cria uma lista com os IDs que deverão ser mantidos no dataframe\n",
    "ids_permitidos = []\n",
    "for i in tabela_tce.index:\n",
    "    kep_id = \"{:09d}\".format(tabela_tce[\"kepid\"][i])\n",
    "    if not kep_id in IDs:\n",
    "        continue\n",
    "    ids_permitidos.append(int(kep_id))\n",
    "\n",
    "# Filtra TCEs existentes na lista IDs\n",
    "tce_existentes = tabela_tce[\"kepid\"].apply(lambda l: l in ids_permitidos)\n",
    "\n",
    "# Tabela apenas com TCEs existentes\n",
    "tabela_tce = tabela_tce[tce_existentes]   \n",
    "\n",
    "coluna_labels = \"av_training_set\"          # Coluna dos labels\n",
    "labels_permitidos = {\"PC\", \"AFP\", \"NTP\"}   # Label permitidos dos TCEs\n",
    "\n",
    "# Filtra TCEs permitidos com base nos labels\n",
    "tce_permitidos = tabela_tce[coluna_labels].apply(lambda l: l in labels_permitidos)\n",
    "\n",
    "# Tabela apenas com TCEs permitidos\n",
    "tabela_tce = tabela_tce[tce_permitidos]\n",
    "\n",
    "tabela_tce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Leitura dos Arquivos .fits --------\n",
    "\n",
    "# Array com os nomes de todos os arquivos .fits baixados\n",
    "nomes_arquivos = []\n",
    "fluxos = []\n",
    "tempos = []\n",
    "kep_ids_str = []\n",
    "\n",
    "kep_ids_int = pd.unique(tabela_tce[\"kepid\"])\n",
    "\n",
    "for i in kep_ids_int:\n",
    "    i = \"{:09d}\".format(i)\n",
    "    kep_ids_str.append(i)\n",
    "    \n",
    "    nome_arq = kepler_io.kepler_filenames(PASTA_DADOS_KEPLER, i)\n",
    "    assert nome_arq, \"Não foi possível encontrar um aquivo .fits com ID {} em {}\".format(i, PASTA_DADOS_KEPLER)\n",
    "    nomes_arquivos.append(nome_arq)\n",
    "    \n",
    "    tempo, fluxo = kepler_io.read_kepler_light_curve(nome_arq)\n",
    "    \n",
    "    tempos.append(tempo)\n",
    "    fluxos.append(fluxo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Execução principal --------\n",
    "\n",
    "g_views = [] # Array onde todas todas as curvas globais serão armazenadas\n",
    "l_views = [] # Array onde todas todas as curvas locais serão armazenadas\n",
    "\n",
    "i = 0 # Contador do processo atual\n",
    "\n",
    "log_arq = \"/log_erros_\" + pasta + \".txt\" # Arquivo para armazenar os logs de erro\n",
    "salva_log = salva_arq + log_arq # Caminho do arquivo para armazenar os logs de erro\n",
    "\n",
    "with open(salva_log, \"w\") as log:\n",
    "    for index, tce in tabela_tce.iterrows():\n",
    "\n",
    "        # Barra de progresso\n",
    "        print(\"Rodada\", i+1, end = '')\n",
    "        kic = \"{:09d}\".format(tce[\"kepid\"])\n",
    "        pc.print_percent_done(i, len(tabela_tce), kic)\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "        try:\n",
    "            # Índice para correspondência dos IDs com os kepid da tabela.\n",
    "            indice = list(kep_ids_int).index(tce[\"kepid\"])\n",
    "\n",
    "            # Lê e processa a curva de luz.\n",
    "            time, flux = pc.processar_curva_luz(tempos[indice], fluxos[indice])\n",
    "\n",
    "            # Lê o período, duração e epoch do TCE na tabela.\n",
    "            # O fator 1/1000 em t0 serve para centralizar o TCE na curva de luz.\n",
    "            period = tce[\"tce_period\"]\n",
    "            duration = tce[\"tce_duration\"]\n",
    "            t0 = tce[\"tce_time0bk\"]/1000\n",
    "\n",
    "            # \"Folda\" a curva de luz e organiza seus dados em ordem cronológica (tempo_min até tempo_max).\n",
    "            time, flux = pc.phase_fold_and_sort_light_curve(time, flux, period, t0)\n",
    "\n",
    "            # Gera as visões globais e locais e as converte em listas\n",
    "            g_view = list(pc.global_view(time, flux, period))\n",
    "            l_view = list(pc.local_view(time, flux, period, duration))\n",
    "            \n",
    "            # Guarda o KeplerID na primeira posição das listas\n",
    "            g_view.insert(0, tce[\"kepid\"])\n",
    "            l_view.insert(0, tce[\"kepid\"])\n",
    "            \n",
    "            # Guarda o label do TCE na segunda posição das listas\n",
    "            g_view.insert(1, tce[\"av_training_set\"])\n",
    "            l_view.insert(1, tce[\"av_training_set\"])\n",
    "\n",
    "            # Armazena os dados das visões globais e locais.\n",
    "            '''''\n",
    "            Para plotar cada visualização, é necessário criar um array de inteiros de tamanho\n",
    "            2001 para a visão global ou 201 para a visão local:\n",
    "\n",
    "                g_bins = np.linspace(0, 2000, num = 2001) # Bins para a visão global\n",
    "                l_bins = np.linspace(0, 200, num = 201) # Bons para a visão local\n",
    "            '''''\n",
    "            g_views.append(g_view)\n",
    "            l_views.append(l_view)\n",
    "\n",
    "        except:\n",
    "            \"\"\"\n",
    "            Esse if checa se g_views recebeu a curva de luz da rodada atual. Se o problema\n",
    "            for levantado apenas em l_views, g_views já vai ter recebido a acurva de luz, mas\n",
    "            l_views não. Assim, as duas listas terão tamanhos diferentes. Para evitar isso, \n",
    "            estou inserindo um 'NaN' na posição que deveria receber uma curva de luz na\n",
    "            visualização local.\n",
    "            \"\"\"\n",
    "            if len(g_views) > len(l_views):\n",
    "                l_views.append(float(\"NaN\"))\n",
    "            \n",
    "            # Escreve e armazena os logs de erro\n",
    "            log.write(\"Problema em KIC {0} na rodada {1}\\n\".format(tce[\"kepid\"], i-1))\n",
    "            traceback.print_exc(file=log)\n",
    "            log.write(\"\\n \\n\")\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Salvamento das Curvas --------\n",
    "\n",
    "# Salva as curvas de luz tratadas em um arquivo binário no formato .pickle\n",
    "\"\"\"\n",
    "Para carregar os arquivos, basta usar algo como:\n",
    "\n",
    "with open(r\"visualizacao.pickle\", \"rb\") as input_file:\n",
    "    visualizacao = cPickle.load(input_file)\n",
    "\n",
    "Vale lembrar que o primeiro elemento das listas é o \n",
    "kepid do TCE e o segundo é o seu label.\n",
    "\"\"\"\n",
    "\n",
    "# Strings com os locais onde devem ser salvos os arquivos\n",
    "arq_g_views = \"/g_views_\" + pasta + \".pickle\"\n",
    "arq_l_views = \"/l_views_\" + pasta + \".pickle\"\n",
    "\n",
    "# Salva as curvas de luz globais\n",
    "with open(salva_arq + arq_g_views, \"wb\") as saida_g_views:\n",
    "    cPickle.dump(g_views, saida_g_views)\n",
    "    print(\"{} curvas de luz globais armazenadas.\".format(len(g_views)))\n",
    "   \n",
    "# Salva as curvas de luz locais \n",
    "with open(salva_arq + arq_l_views, \"wb\") as saida_l_views:\n",
    "    cPickle.dump(l_views, saida_l_views)\n",
    "    print(\"{} curvas de luz locais armazenadas.\".format(len(l_views)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------- Teste --------\n",
    "g_bins = np.linspace(0, 2000, num = 2001) # Bins para a visão global\n",
    "l_bins = np.linspace(0, 200, num = 201) # Bons para a visão local\n",
    "\n",
    "curva = 0\n",
    "\n",
    "print(g_views[curva][1])\n",
    "plt.plot(g_bins, g_views[curva][2:], \".\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(l_bins, l_views[curva][2:], \".\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Seleciona a pasta raiz onde as pastas 'kepler_n' estão\n",
    "raiz = \"insira/o/caminho\"\n",
    "\n",
    "# Informa ao script até que pasta você quer trabalhar.\n",
    "# Insira o valor (se você seguiu o meu exemplo) para usar todo o dataset.\n",
    "# Valores menores que isso são para testes.\n",
    "# Deve ser um valor inteiro e maior que 0.\n",
    "n = 2\n",
    "\n",
    "# Lê todas as curvas de luz tratadas que foram selecionadas\n",
    "for item in range(n):\n",
    "    pasta_proc = raiz + str(item+1)\n",
    "    #print(\"kep_\" + str(item+1), end = \" \")\n",
    "    \n",
    "    arq_g_view = pasta_proc + \"/g_views_\" + str(item+1) + \".pickle\"\n",
    "    arq_l_view = pasta_proc + \"/l_views_\" + str(item+1) + \".pickle\"\n",
    "    \n",
    "    if item == 0:\n",
    "        with open(arq_g_view, \"rb\") as input_file:\n",
    "            g_views = cPickle.load(input_file)\n",
    "        print(\"Global - Rodada: \" + str(item+1), input_file.closed)\n",
    "            \n",
    "        with open(arq_l_view, \"rb\") as input_file:\n",
    "            l_views = cPickle.load(input_file)\n",
    "        print(\"Local  - Rodada: \" + str(item+1), input_file.closed)\n",
    "    else:\n",
    "        with open(arq_g_view, \"rb\") as input_file:\n",
    "            g_views += cPickle.load(input_file)\n",
    "        print(\"Global - Rodada: \" + str(item+1), input_file.closed)\n",
    "            \n",
    "        with open(arq_l_view, \"rb\") as input_file:\n",
    "            l_views += cPickle.load(input_file)\n",
    "        print(\"Local  - Rodada: \" + str(item+1), input_file.closed)\n",
    "\n",
    "\n",
    "# Faz uma correspondência entre as curva global com a respectiva local.\n",
    "curvas = list(zip(g_views, l_views))\n",
    "\n",
    "# Embaralha os elementos de cada lista.\n",
    "#   Estabelecer uma raiz e criar uma nova lista com os mesmos elementos\n",
    "#   das demais listas (mas embaralhados) é puro preciosismo da minha parte.\n",
    "random.seed(23)\n",
    "random.shuffle(curvas)\n",
    "\n",
    "# Retorna os valores embaralhados para as listas originais.\n",
    "g_views, l_views = zip(*curvas)\n",
    "\n",
    "# Partição da tabela dos TCEs como segue:\n",
    "#   tces_treino = 80% dos TCEs\n",
    "#   tces_val    = 10% dos TCEs\n",
    "#   tces_teste  = 10% dos TCEs\n",
    "num_tces = len(g_views)\n",
    "\n",
    "corte_treino = int(0.80 * num_tces)\n",
    "corte_val = int(0.90 * num_tces)\n",
    "\n",
    "g_treino = g_views[0:corte_treino]\n",
    "l_treino = l_views[0:corte_treino]\n",
    "\n",
    "g_val = g_views[corte_treino:corte_val]\n",
    "l_val = l_views[corte_treino:corte_val]\n",
    "\n",
    "g_teste = g_views[corte_val:]\n",
    "l_teste = l_views[corte_val:]\n",
    "\n",
    "\n",
    "# Pasta para salvar os arquivos de treino, validação e teste\n",
    "salva_arq = \"insira/o/caminho\"\n",
    "\n",
    "#   Cria a pasta para armazenar os arquivos salvos caso não exista.\n",
    "if not os.path.exists(salva_arq):\n",
    "    os.makedirs(salva_arq)\n",
    "\n",
    "salva_ML_arq = salva_arq + \"/g_treino.pickle\"\n",
    "with open(salva_ML_arq, \"wb\") as arq_ML:\n",
    "    cPickle.dump(g_treino, arq_ML)\n",
    "print(\"Arquivo g_treino.pickle salvo com sucesso.\")\n",
    "\n",
    "salva_ML_arq = salva_arq + \"/l_treino.pickle\"\n",
    "with open(salva_ML_arq, \"wb\") as arq_ML:\n",
    "    cPickle.dump(l_treino, arq_ML)\n",
    "print(\"Arquivo l_treino.pickle salvo com sucesso.\")\n",
    "\n",
    "salva_ML_arq = salva_arq + \"/g_val.pickle\"\n",
    "with open(salva_ML_arq, \"wb\") as arq_ML:\n",
    "    cPickle.dump(g_val, arq_ML)\n",
    "print(\"Arquivo g_val.pickle salvo com sucesso.\")\n",
    "\n",
    "salva_ML_arq = salva_arq + \"/l_val.pickle\"\n",
    "with open(salva_ML_arq, \"wb\") as arq_ML:\n",
    "    cPickle.dump(l_val, arq_ML)\n",
    "print(\"Arquivo l_val.pickle salvo com sucesso.\")\n",
    "\n",
    "salva_ML_arq = salva_arq + \"/g_teste.pickle\"\n",
    "with open(salva_ML_arq, \"wb\") as arq_ML:\n",
    "    cPickle.dump(g_teste, arq_ML)\n",
    "print(\"Arquivo g_teste.pickle salvo com sucesso.\")\n",
    "\n",
    "salva_ML_arq = salva_arq + \"/l_teste.pickle\"\n",
    "with open(salva_ML_arq, \"wb\") as arq_ML:\n",
    "    cPickle.dump(l_teste, arq_ML)\n",
    "print(\"Arquivo l_teste.pickle salvo com sucesso.\")\n",
    "\n",
    "\n",
    "print(\"Fim do processamento.\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "809ea8a60c312ae28f0276f5e6f3dc4b8e3bad8df1be3505e645c844e27d5f02"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
